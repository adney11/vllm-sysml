from vllm import LLM, SamplingParams
import csv

import torch
torch.cuda.empty_cache()

import os
import pathlib
tests_path = pathlib.Path(__file__).parent.resolve()

# For dlprof and pytorch
# import nvidia_dlprof_pytorch_nvtx
# nvidia_dlprof_pytorch_nvtx.init()

prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]

# Reads from random_prompts.csv (generated by utils.py) to grab pre-generated random input tokens of n length
# where n = 128, 256, 512, 1024, 2048, 4096, 8192
n_token_prompts = {}
with open(os.path.join(tests_path, "random_prompts.csv"), "r") as fr:
    cr = csv.reader(fr, delimiter=',')
    for r in cr:
        r_int = [int(t) for t in r]
        n_token_prompts[r_int[0]] = r_int[1:]

# Change [n] to compare. Different models have different upper token limits.
prompt_token_ids = [
    n_token_prompts[1024]
]

sampling_params = SamplingParams(n=1, temperature=0, top_p=1, top_k=-1, min_p=0, ignore_eos=True) # make deterministic: temperature=0 for greedy sampling, top_p=1 for consider all tokens, top_k=-1 to consider all tokens, min_p=0 to disable

# Uncomment which model you want to test with
llm = LLM(model="facebook/opt-125m")
# llm = LLM(model="meta-llama/Llama-2-7b-hf")

# For dlprof and pytorch
# with torch.autograd.profiler.emit_nvtx():
    # outputs = llm.generate(prompts, sampling_params)

outputs = llm.generate(sampling_params=sampling_params, prompt_token_ids=prompt_token_ids)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
