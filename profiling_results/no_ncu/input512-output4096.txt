

Input: 512
Output: 4096


INFO 03-23 20:31:24 llm_engine.py:74] Initializing an LLM engine with config: model='meta-llama/llama-2-7b-hf', tokenizer='meta-llama/llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-23 20:31:27 weight_utils.py:163] Using model weights format ['*.safetensors']
INFO 03-23 20:31:31 llm_engine.py:368] # GPU blocks: 817, # CPU blocks: 512
INFO 03-23 20:31:32 model_runner.py:671] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-23 20:31:32 model_runner.py:675] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-23 20:31:35 model_runner.py:737] Graph capturing finished in 3 secs.
INFO 03-23 20:31:35 llm_engine.py:125] 
INFO 03-23 20:31:35 llm_engine.py:126] CACHE CONFIG INIT
INFO 03-23 20:31:35 llm_engine.py:127] block_size = 16
INFO 03-23 20:31:35 llm_engine.py:128] gpu_memory_utilization = 0.9
INFO 03-23 20:31:35 llm_engine.py:129] cpu_swap_space_bytes = 4294967296 bytes
INFO 03-23 20:31:35 llm_engine.py:130] cache_dtype = auto
INFO 03-23 20:31:35 llm_engine.py:131] sliding_window = None
INFO 03-23 20:31:35 llm_engine.py:132] num_gpu_blocks = 817
INFO 03-23 20:31:35 llm_engine.py:133] num_cpu_blocks = 512
INFO 03-23 20:31:35 llm_engine.py:134] 
===> Start of PROMPT phase: Begin profiling...
INFO 03-23 20:31:35 llm_engine.py:849] 
INFO 03-23 20:31:35 llm_engine.py:849] ===! Step 0 is prompt phase !===
INFO 03-23 20:31:35 llm_engine.py:849] 
===> End of PROMPT phase: Stop profiling...
INFO 03-23 20:31:35 metrics.py:213] 
INFO 03-23 20:31:35 metrics.py:214] Avg prompt throughput: 4373.01 tokens/s 
INFO 03-23 20:31:35 metrics.py:215] Avg generation throughput: 0.00 tokens/s
INFO 03-23 20:31:35 metrics.py:216] Avg Running: 1.00 reqs
INFO 03-23 20:31:35 metrics.py:217] Avg Swapped: 0.00 reqs
INFO 03-23 20:31:35 metrics.py:218] Avg Pending: 0.00 reqs
INFO 03-23 20:31:35 metrics.py:219] Avg GPU KV cache usage: 3.92%
INFO 03-23 20:31:35 metrics.py:220] Avg CPU KV cache usage: 0.00%
INFO 03-23 20:31:35 metrics.py:221] Avg time_to_first_tokens: 0.116978s
INFO 03-23 20:31:35 metrics.py:222] Avg time_per_output_tokens: 0.000000s
INFO 03-23 20:31:35 metrics.py:223] Avg time_e2e_requests: 0.000000s
INFO 03-23 20:31:35 metrics.py:224] 
===> Start of TOKEN phase: Begin profiling...
===> End of TOKEN phase: Stop profiling...
INFO 03-23 20:33:31 metrics.py:213] 
INFO 03-23 20:33:31 metrics.py:214] Avg prompt throughput: 0.00 tokens/s 
INFO 03-23 20:33:31 metrics.py:215] Avg generation throughput: 30.98 tokens/s
INFO 03-23 20:33:31 metrics.py:216] Avg Running: 1.00 reqs
INFO 03-23 20:33:31 metrics.py:217] Avg Swapped: 0.00 reqs
INFO 03-23 20:33:31 metrics.py:218] Avg Pending: 0.00 reqs
INFO 03-23 20:33:31 metrics.py:219] Avg GPU KV cache usage: 17.67%
INFO 03-23 20:33:31 metrics.py:220] Avg CPU KV cache usage: 0.00%
INFO 03-23 20:33:31 metrics.py:221] Avg time_to_first_tokens: 0.000000s
INFO 03-23 20:33:31 metrics.py:222] Avg time_per_output_tokens: 0.032280s
INFO 03-23 20:33:31 metrics.py:223] Avg time_e2e_requests: 115.842058s
INFO 03-23 20:33:31 metrics.py:224] 
Request ID: 0                       # Input Tokens: 511                       # Output Tokens: 3586                       Finish Reason: length

===! Num steps executed: 3586 !===

