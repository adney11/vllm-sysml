

Input: 8192
Output: 512


INFO 03-23 20:51:21 llm_engine.py:74] Initializing an LLM engine with config: model='meta-llama/llama-2-7b-hf', tokenizer='meta-llama/llama-2-7b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-23 20:51:24 weight_utils.py:163] Using model weights format ['*.safetensors']
INFO 03-23 20:51:27 llm_engine.py:368] # GPU blocks: 817, # CPU blocks: 512
INFO 03-23 20:51:29 model_runner.py:671] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-23 20:51:29 model_runner.py:675] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-23 20:51:32 model_runner.py:737] Graph capturing finished in 3 secs.
INFO 03-23 20:51:32 llm_engine.py:125] 
INFO 03-23 20:51:32 llm_engine.py:126] CACHE CONFIG INIT
INFO 03-23 20:51:32 llm_engine.py:127] block_size = 16
INFO 03-23 20:51:32 llm_engine.py:128] gpu_memory_utilization = 0.9
INFO 03-23 20:51:32 llm_engine.py:129] cpu_swap_space_bytes = 4294967296 bytes
INFO 03-23 20:51:32 llm_engine.py:130] cache_dtype = auto
INFO 03-23 20:51:32 llm_engine.py:131] sliding_window = None
INFO 03-23 20:51:32 llm_engine.py:132] num_gpu_blocks = 817
INFO 03-23 20:51:32 llm_engine.py:133] num_cpu_blocks = 512
INFO 03-23 20:51:32 llm_engine.py:134] 
===> Start of PROMPT phase: Begin profiling...
WARNING 03-23 20:51:32 scheduler.py:205] Input prompt (8191 tokens) is too long and exceeds limit of 4096
